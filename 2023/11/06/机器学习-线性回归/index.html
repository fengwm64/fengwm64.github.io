<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="1.原理1.1 闭式求解 模型：$h_\theta(x)&#x3D;\theta^TX$  损失函数：$J(\theta)&#x3D;\left|X\theta-Y\right|_2^2$  目标：$\theta&#x3D;\arg\min J(\theta)$  说明：    $$ \begin{cases}\theta\in\mathbb{R}^{d\times1}\\[2ex]X\in\ma">
<meta property="og:type" content="article">
<meta property="og:title" content="[机器学习]线性回归推导">
<meta property="og:url" content="https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="迎风起降的小站">
<meta property="og:description" content="1.原理1.1 闭式求解 模型：$h_\theta(x)&#x3D;\theta^TX$  损失函数：$J(\theta)&#x3D;\left|X\theta-Y\right|_2^2$  目标：$\theta&#x3D;\arg\min J(\theta)$  说明：    $$ \begin{cases}\theta\in\mathbb{R}^{d\times1}\\[2ex]X\in\ma">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/fengwm64/imgs/imgs/202404241128729.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/fengwm64/imgs/imgs/202404241143950.png">
<meta property="article:published_time" content="2023-11-06T04:19:44.000Z">
<meta property="article:modified_time" content="2025-02-03T17:34:43.706Z">
<meta property="article:author" content="fengwm">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="线性回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/fengwm64/imgs/imgs/202404241128729.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>[机器学习]线性回归推导</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="顶部" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/search/">搜索</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇" href="/2023/12/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇" href="/2023/04/22/%E9%83%A8%E7%BD%B2-wsl2-bochs%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&text=[机器学习]线性回归推导"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&is_video=false&description=[机器学习]线性回归推导"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=[机器学习]线性回归推导&body=Check out this article: https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&name=[机器学习]线性回归推导&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&t=[机器学习]线性回归推导"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">1.原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E9%97%AD%E5%BC%8F%E6%B1%82%E8%A7%A3"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 闭式求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B1%82%E8%A7%A3"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 梯度下降求解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Python%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">2.Python实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-0-%E5%AF%BC%E5%8C%85"><span class="toc-number">2.1.</span> <span class="toc-text">2.0 导包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.2.</span> <span class="toc-text">2.1 读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.3.</span> <span class="toc-text">2.2 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.4.</span> <span class="toc-text">2.3 划分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%B1%82%E8%A7%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.5.</span> <span class="toc-text">2.4 求解模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-0-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87R-2"><span class="toc-number">2.5.1.</span> <span class="toc-text">2.4.0 评价指标R^2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-1-%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">2.5.2.</span> <span class="toc-text">2.4.1 数据标准化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-2-%E9%97%AD%E5%90%88%E5%BD%A2%E5%BC%8F%E6%B1%82%E8%A7%A3"><span class="toc-number">2.5.3.</span> <span class="toc-text">2.4.2 闭合形式求解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B1%82%E8%A7%A3"><span class="toc-number">2.5.4.</span> <span class="toc-text">2.4.3 梯度下降求解</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text">3.实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%8E%A2%E7%B4%A2%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 探索数据特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%B1%82%E8%A7%A3%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 求解结果展示</span></a></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        [机器学习]线性回归推导
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">fengwm</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2023-11-06T04:19:44.000Z" class="dt-published" itemprop="datePublished">2023-11-06</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
    </div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>, <a class="p-category" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h2 id="1-原理"><a href="#1-原理" class="headerlink" title="1.原理"></a>1.原理</h2><h3 id="1-1-闭式求解"><a href="#1-1-闭式求解" class="headerlink" title="1.1 闭式求解"></a>1.1 闭式求解</h3><ul>
<li><p><strong>模型：</strong>$h_\theta(x)&#x3D;\theta^TX$</p>
</li>
<li><p><strong>损失函数：</strong>$J(\theta)&#x3D;\left|X\theta-Y\right|_2^2$</p>
</li>
<li><p><strong>目标：</strong>$\theta&#x3D;\arg\min J(\theta)$</p>
</li>
<li><p><strong>说明：</strong></p>
</li>
</ul>

$$
\begin{cases}\theta\in\mathbb{R}^{d\times1}\\[2ex]X\in\mathbb{R}^{m\times d}\\[2ex]Y\in\mathbb{R}^{m\times1}\end{cases}
$$


<p>正规方程形式求解，即为直接求 $J(\theta)$ 的最小值：</p>
<p>先展开 $J(\theta)$ ：</p>

$$\begin{align*}
J(\theta) &= \|X\theta - Y\|_{2}^{2} \\
&= (X\theta - Y)^{T}(X\theta - Y) \\
&= (X^{T}\theta^{T} - Y^{T})(X\theta - Y) \\
&= X^{T}\theta^{T}X\theta - Y^{T}X\theta - Y^{T}X\theta + Y^{T}Y \\
&= X^{T}\theta^{T}X\theta - 2Y^{T}X\theta + Y^{T}Y 
\end{align*}
$$


<p>对 $J(\theta)$ 进行求导：</p>

$$\begin{aligned}
\frac{\partial J(\theta)}{\partial\theta}& =\frac{\partial X^T\theta^TX\theta-2Y^TX\theta+Y^TY}{\partial\theta} \\
&=2X^{T}X\theta-2Y^{T}X
\end{aligned}$$


<p>令 $J(\theta)&#x3D;0$ 得：</p>

$$\begin{aligned}
2X^{T}X\theta-2Y^{T}X& =0 \\
X^{T}X\theta & =Y^{T}X \\
\theta & =(X^TX)^{-1}Y^TX 
\end{aligned}$$


<p>上述结果即为求解结果，需要说明的是：特征矩阵 $X$ 不满秩（即存在特征间的线性相关性），则正规方程求解过程中的矩阵求逆操作可能会导致数值不稳定性。</p>
<h3 id="1-2-梯度下降求解"><a href="#1-2-梯度下降求解" class="headerlink" title="1.2 梯度下降求解"></a>1.2 梯度下降求解</h3><ul>
<li><p><strong>模型：</strong>$h_\theta(x)&#x3D;\sum_{i&#x3D;1}^d\theta_ix_i$ </p>
<p>  注：$x_i$表示$x$的第$i$维</p>
</li>
<li><p><strong>损失函数：</strong>$J(\theta)&#x3D;\frac1{2m}\sum_{j&#x3D;0}^m\left(y^j-h_\theta(x^j)\right)^2$</p>
<p>  注：$x^j$表示第$j$个样本</p>
</li>
<li><p><strong>目标：</strong>$\theta&#x3D;\arg\min J(\theta)$</p>
</li>
<li><p><strong>说明：</strong></p>
</li>
</ul>

$$
\begin{cases}\theta\in\mathbb{R}^d\\[2ex]x\in\mathbb{R}^d\\[2ex]y\in\mathbb{R}^m\end{cases}
$$


<p>损失函数 $J(\theta)$ 是一个关于参数 $\theta$ 的二次型，对 $J(\theta)$ 进行展开：</p>

$$\begin{aligned}
J(\theta)& =\frac{1}{2m}\sum_{j=0}^{m}\Big(y^{j}-h_{\theta}(x^{j})\Big) \\
&=\frac{1}{2m}\sum_{j=0}^{m}\Bigg(y^{j}-\sum_{i=1}^{d}\theta_{i}x_{i}^{j}\Bigg)^{2}
\end{aligned}$$


<p>对 $J(\theta)$ 进行偏微分求导运算得到：</p>

$$\begin{aligned}
\partial\frac{J(\theta)}{\partial\theta_i}& =\frac{\partial}{\partial\theta_{i}}\frac{1}{2m}\sum_{j=0}^{m}\Bigg(y^{j}-\sum_{i=1}^{d}\theta_{i}x_{i}^{j}\Bigg)^{2} \\
&=\frac{1}{m}\sum_{j=0}^{m}\Bigg( y^{j}-\sum_{i=1}^{d}\theta_{i}x_{i}^{j}\Bigg)(-x_{i}^{j}) \\
&=\frac{1}{m}\sum_{j=0}^{m}\Bigg(\sum_{i=1}^{d}\theta_{i}x_{i}^{j}-y^{j}\Bigg)x_{i}^{j}
\end{aligned}$$


<p>每次根据梯度更新参数：</p>

$$\begin{aligned}
\theta_{i}& =\theta_i-\alpha\partial\frac{J(\theta)}{\partial\theta_i} \\
&=\theta_i-\alpha(\frac1m\sum_{j=0}^m\biggl(\sum_{i=1}^d\theta_ix_i^j-y^j\biggr)x_i^j) \\
&=\theta_i+\alpha \frac{1}{m}\sum_{j=0}^m\Bigg( y^j-\sum_{i=1}^d\theta_ix_i^j\Bigg)x_i^j
\end{aligned}$$


<p>梯度下降法步骤：</p>

$\text{Repeat until convergence } \{$
$$\theta_i:=\theta_i+\alpha\:\frac{1}{m}\sum_{j=0}^m\Bigg(y^j-\sum_{i=1}^d\theta_ix_i^j\Bigg)x_i^j$$
$\}$


<h2 id="2-Python实现"><a href="#2-Python实现" class="headerlink" title="2.Python实现"></a>2.Python实现</h2><h3 id="2-0-导包"><a href="#2-0-导包" class="headerlink" title="2.0 导包"></a>2.0 导包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-1-读取数据集"><a href="#2-1-读取数据集" class="headerlink" title="2.1 读取数据集"></a>2.1 读取数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">df = pd.read_csv(<span class="string">&quot;./housing.csv&quot;</span>)</span><br><span class="line"><span class="comment"># 预览数据</span></span><br><span class="line"><span class="built_in">print</span>(df.head())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(df.info())</span><br></pre></td></tr></table></figure>

<h3 id="2-2-数据预处理"><a href="#2-2-数据预处理" class="headerlink" title="2.2 数据预处理"></a>2.2 数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1）处理缺失值</span></span><br><span class="line"><span class="comment"># 取出有缺失值的列</span></span><br><span class="line"><span class="comment"># reshape是为了适应sklearn要求</span></span><br><span class="line">total_bedrooms = df.loc[:, <span class="string">&quot;total_bedrooms&quot;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制一份不破坏原数据</span></span><br><span class="line">filled_df = df.copy()  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 中位数填补</span></span><br><span class="line">filled_df.loc[:, <span class="string">&quot;total_bedrooms&quot;</span>] = SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>).fit_transform(total_bedrooms)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一下效果</span></span><br><span class="line">filled_df.info()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2）编码</span></span><br><span class="line"><span class="comment"># 编码</span></span><br><span class="line">code = OneHotEncoder().fit_transform(filled_df.loc[:, <span class="string">&quot;ocean_proximity&quot;</span>].values.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并</span></span><br><span class="line">coded_df = pd.concat([filled_df, pd.DataFrame(code.toarray())], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除原列</span></span><br><span class="line">coded_df.drop([<span class="string">&quot;ocean_proximity&quot;</span>], axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改下表头</span></span><br><span class="line">coded_df.columns = <span class="built_in">list</span>(coded_df.columns[:-<span class="number">5</span>]) + [<span class="string">&quot;ocean_0&quot;</span>, <span class="string">&quot;ocean_1&quot;</span>, <span class="string">&quot;ocean_2&quot;</span>, <span class="string">&quot;ocean_3&quot;</span>, <span class="string">&quot;ocean_4&quot;</span>]</span><br><span class="line"><span class="comment"># coded_df.columns = coded_df.columns.astype(str)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 看看效果</span></span><br><span class="line">coded_df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-划分数据集"><a href="#2-3-划分数据集" class="headerlink" title="2.3 划分数据集"></a>2.3 划分数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">feature = coded_df.iloc[:, :<span class="number">8</span>].join(coded_df.iloc[:, -<span class="number">5</span>:])</span><br><span class="line">label = coded_df[<span class="string">&quot;median_house_value&quot;</span>]</span><br><span class="line"></span><br><span class="line">Xtrain,Xtest,Ytrain,Ytest = train_test_split(feature,label,test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">Xtrain.head()</span><br></pre></td></tr></table></figure>

<h3 id="2-4-求解模型"><a href="#2-4-求解模型" class="headerlink" title="2.4 求解模型"></a>2.4 求解模型</h3><h4 id="2-4-0-评价指标R-2"><a href="#2-4-0-评价指标R-2" class="headerlink" title="2.4.0 评价指标R^2"></a>2.4.0 评价指标R^2</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算R^2</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">R2</span>(<span class="params">y, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - (np.<span class="built_in">sum</span>((y - y_pred) ** <span class="number">2</span>) / np.<span class="built_in">sum</span>((y - np.mean(y)) ** <span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<h4 id="2-4-1-数据标准化"><a href="#2-4-1-数据标准化" class="headerlink" title="2.4.1 数据标准化"></a>2.4.1 数据标准化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize</span>(<span class="params">X</span>):</span><br><span class="line">    sigma = np.std(X, axis=<span class="number">0</span>)</span><br><span class="line">    mu = np.mean(X, axis=<span class="number">0</span>)</span><br><span class="line">    X = (X - mu) / sigma</span><br><span class="line">    <span class="keyword">return</span> np.array(X)</span><br><span class="line"></span><br><span class="line">X = np.array(Xtrain).reshape(np.size(Xtrain, <span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">y = np.array(Ytrain).T.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化（闭式求解其实不需要，但梯度下降需要，为了对比统一都采用归一化）</span></span><br><span class="line">X = normalize(X)</span><br><span class="line">y = normalize(y)</span><br></pre></td></tr></table></figure>

<h4 id="2-4-2-闭合形式求解"><a href="#2-4-2-闭合形式求解" class="headerlink" title="2.4.2 闭合形式求解"></a>2.4.2 闭合形式求解</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1）线性回归模型的闭合形式参数求解</span></span><br><span class="line"><span class="comment"># 正规方程求解</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Normal_Equation</span>(<span class="params">X, y</span>):</span><br><span class="line">    <span class="keyword">return</span> np.linalg.inv(X.T @ X) @ X.T @ y</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">theta_ne = Normal_Equation(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;花费时间：<span class="subst">&#123;time.time() - start_time&#125;</span>&quot;</span>)v</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;R^2：<span class="subst">&#123;R2(y, X @ theta_ne)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DataFrame</span></span><br><span class="line">result_cf = pd.DataFrame(&#123;<span class="string">&quot;ColumnName&quot;</span>: <span class="built_in">list</span>(Xtrain.columns), <span class="string">&quot;Theta&quot;</span>: theta_ne.flatten()&#125;)</span><br><span class="line">result_cf</span><br></pre></td></tr></table></figure>

<h4 id="2-4-3-梯度下降求解"><a href="#2-4-3-梯度下降求解" class="headerlink" title="2.4.3 梯度下降求解"></a>2.4.3 梯度下降求解</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2）线性回归梯度下降参数求解</span></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">MSE_Loss</span>(<span class="params">y, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>((y_pred - y) ** <span class="number">2</span>) / (<span class="number">2</span> * np.size(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GD</span>(<span class="params">X, y, lr=<span class="number">0.01</span>, epochs=<span class="number">5000</span></span>):</span><br><span class="line">    m, n = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数为标准正态分布</span></span><br><span class="line">    theta = np.random.randn(n, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 记录每代损失</span></span><br><span class="line">    loss = np.zeros(epochs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="comment"># 计算梯度</span></span><br><span class="line">        gradient = (<span class="number">1</span> / m) * (X.T @ (X @ theta - y))</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        theta -= lr * gradient</span><br><span class="line">        <span class="comment"># 记录损失</span></span><br><span class="line">        loss[epoch] = MSE_Loss(y, X @ theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, loss</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">[theta_gd, loss] = GD(X, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;花费时间：<span class="subst">&#123;time.time() - start_time&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;R^2：<span class="subst">&#123;R2(y, X @ theta_gd)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DataFrame</span></span><br><span class="line">result_gd = pd.DataFrame(&#123;<span class="string">&quot;ColumnName&quot;</span>: <span class="built_in">list</span>(Xtrain.columns), <span class="string">&quot;Theta&quot;</span>: theta_gd.flatten()&#125;)</span><br><span class="line">result_gd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失函数梯度下降曲线</span></span><br><span class="line">sns.lineplot(x=np.arange(<span class="number">5000</span>), y=loss.flatten(), label=<span class="string">&#x27;Loss Curve&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Gradient Descent Loss Curve&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3.实验结果"></a>3.实验结果</h2><h3 id="3-1-探索数据特征"><a href="#3-1-探索数据特征" class="headerlink" title="3.1 探索数据特征"></a>3.1 探索数据特征</h3><p><img src="https://cdn.jsdelivr.net/gh/fengwm64/imgs/imgs/202404241128729.png"></p>
<h3 id="3-2-求解结果展示"><a href="#3-2-求解结果展示" class="headerlink" title="3.2 求解结果展示"></a>3.2 求解结果展示</h3><p><img src="https://cdn.jsdelivr.net/gh/fengwm64/imgs/imgs/202404241143950.png"></p>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>加载评论需要在浏览器启用 JavaScript 脚本支持。</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/about/">关于</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">1.原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E9%97%AD%E5%BC%8F%E6%B1%82%E8%A7%A3"><span class="toc-number">1.1.</span> <span class="toc-text">1.1 闭式求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B1%82%E8%A7%A3"><span class="toc-number">1.2.</span> <span class="toc-text">1.2 梯度下降求解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Python%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">2.Python实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-0-%E5%AF%BC%E5%8C%85"><span class="toc-number">2.1.</span> <span class="toc-text">2.0 导包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.2.</span> <span class="toc-text">2.1 读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.3.</span> <span class="toc-text">2.2 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.4.</span> <span class="toc-text">2.3 划分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%B1%82%E8%A7%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.5.</span> <span class="toc-text">2.4 求解模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-0-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87R-2"><span class="toc-number">2.5.1.</span> <span class="toc-text">2.4.0 评价指标R^2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-1-%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">2.5.2.</span> <span class="toc-text">2.4.1 数据标准化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-2-%E9%97%AD%E5%90%88%E5%BD%A2%E5%BC%8F%E6%B1%82%E8%A7%A3"><span class="toc-number">2.5.3.</span> <span class="toc-text">2.4.2 闭合形式求解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B1%82%E8%A7%A3"><span class="toc-number">2.5.4.</span> <span class="toc-text">2.4.3 梯度下降求解</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text">3.实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%8E%A2%E7%B4%A2%E6%95%B0%E6%8D%AE%E7%89%B9%E5%BE%81"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 探索数据特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%B1%82%E8%A7%A3%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 求解结果展示</span></a></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&text=[机器学习]线性回归推导"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&is_video=false&description=[机器学习]线性回归推导"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=[机器学习]线性回归推导&body=Check out this article: https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&title=[机器学习]线性回归推导"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&name=[机器学习]线性回归推导&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://blog.fnas64.us.kg/2023/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/&t=[机器学习]线性回归推导"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2023-2025
    fengwm
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/about/">关于</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/search/">搜索</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

  <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?04e4c4fe8e8ea98b2f25d13be30afad8";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>

<!-- Cloudflare Analytics -->

  <script defer src='https://static.cloudflareinsights.com/beacon.min.js' data-cf-beacon='{"token": "da51e59ed20f49fbb866761788bcbd06"}'></script>

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = 'fengwm64/fengwm64.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'Comment';
      var utterances_theme = 'github-light';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
